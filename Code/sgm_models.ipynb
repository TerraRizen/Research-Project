{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# μPyD‑Net - Peluoso et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Conv2DTranspose, Concatenate, Lambda, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TimeDistributed, ConvLSTM2D\n",
    "import visualkeras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen Split (Eigen et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_file(split_file_path, data_root, target_size=(64, 64), original_width=1242):\n",
    "    \"\"\"\n",
    "    Loads images and disparity maps based on a provided split file.\n",
    "    Applies same logic for train, test, and validation splits.\n",
    "    \"\"\"\n",
    "    with open(split_file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    images = []\n",
    "    disparities = []\n",
    "    sequences = set()\n",
    "    scale_factor = target_size[1] / original_width\n",
    "\n",
    "    for line in lines:\n",
    "        left_path = line.strip().split()[0]\n",
    "        parts = Path(left_path).parts[1:]\n",
    "        rel_path = Path(*parts)\n",
    "\n",
    "        sequence = rel_path.parts[0]\n",
    "        filename = rel_path.stem\n",
    "\n",
    "        image_path = Path(data_root) / sequence / \"images\" / f\"{filename}.png\"\n",
    "        disp_path = Path(data_root) / sequence / \"depths\" / f\"{filename}.npy\"\n",
    "        sequences.add(sequence)\n",
    "\n",
    "        if not image_path.exists() or not disp_path.exists():\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, target_size).astype(np.float32) / 255.0\n",
    "        img = img[..., np.newaxis]\n",
    "\n",
    "        disp = np.load(disp_path)\n",
    "        disp = cv2.resize(disp, target_size, interpolation=cv2.INTER_NEAREST).astype(np.float32)\n",
    "        disp = disp * scale_factor\n",
    "        disp = disp[..., np.newaxis]\n",
    "\n",
    "        images.append(img)\n",
    "        disparities.append(disp)\n",
    "\n",
    "    return np.array(images), np.array(disparities), sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your paths to the preprocessed dataset.\n",
    "data_root = r\"\"\n",
    "train_file = r\"\"\n",
    "val_file   = r\"\"\n",
    "test_file = r\"\"\n",
    "\n",
    "\n",
    "X_train_2, y_train_2, train_sequences = load_split_file(train_file, data_root, target_size=(32, 32))\n",
    "X_val_2,   y_val_2,   val_sequences   = load_split_file(val_file, data_root, target_size=(32, 32))\n",
    "X_test_2,  y_test_2,  test_sequences  = load_split_file(test_file, data_root, target_size=(32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should show 22600, 888, 697. Otherwise, something went wrong.\n",
    "print(len(X_train_2))\n",
    "print(len(X_val_2))\n",
    "print(len(X_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(X, y, index=600):\n",
    "    \"\"\"\n",
    "    Visualize a grayscale input image and its corresponding SGM disparity map.\n",
    "    \"\"\"\n",
    "    img = X[index].squeeze()\n",
    "    disp = y[index].squeeze()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(\"Input Grayscale Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(disp, cmap='inferno')\n",
    "    plt.title(\"SGM Disparity Map\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(X_train_2, y_train_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## μPyD‑Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uPyDNet(input_shape=(32, 32, 1)):\n",
    "    def conv_block(x, filters, stride=1):\n",
    "        x = Conv2D(filters, 3, stride, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.125)(x)\n",
    "        return x\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    skips = []\n",
    "\n",
    "    x = conv_block(inputs, 8)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block(x, 8, stride=2)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block(x, 16)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block(x, 16, stride=2)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block(x, 32)\n",
    "\n",
    "    x = Conv2DTranspose(32, 2, strides=2, padding='same')(x)\n",
    "    skip = skips[2]\n",
    "    skip = Reshape((16, 16, 16))(skip)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = conv_block(x, 32)\n",
    "    x = conv_block(x, 32)\n",
    "    x = conv_block(x, 32)\n",
    "\n",
    "    x = Conv2DTranspose(16, 2, strides=2, padding='same')(x)\n",
    "    skip = skips[0]\n",
    "    skip = Reshape((32, 32, 8))(skip)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = conv_block(x, 16)\n",
    "    x = conv_block(x, 16)\n",
    "    x = conv_block(x, 16)\n",
    "\n",
    "    output = Conv2D(1, 3, padding='same')(x)\n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berHu_loss(y_true, y_pred, alpha=0.2):\n",
    "    abs_error = tf.abs(y_true - y_pred)\n",
    "    max_val = tf.reduce_max(abs_error, axis=[1, 2, 3], keepdims=True)\n",
    "    c = alpha * max_val + 1e-6\n",
    "    condition = abs_error <= c\n",
    "    l1 = abs_error\n",
    "    l2 = (tf.square(abs_error) + tf.square(c)) / (2 * c)\n",
    "    return tf.reduce_mean(tf.where(condition, l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_uPyDNet(input_shape=(32, 32, 1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=berHu_loss, metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "color_map = defaultdict(dict)\n",
    "color_map[Conv2D]['fill'] = 'skyblue'\n",
    "color_map[Conv2DTranspose]['fill'] = 'lightgreen'\n",
    "color_map[LeakyReLU]['fill'] = 'orange'\n",
    "color_map[Concatenate]['fill'] = 'pink'\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, color_map=color_map, to_file='upyDNet_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training μPyD‑Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=80, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=50, factor=0.7, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_2, y_train_2,\n",
    "    validation_data=(X_val_2, y_val_2),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set evaluation (Monodepth Eigen test set - Godard et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae = model.evaluate(X_test_2, y_test_2)\n",
    "print(f\"\\nTest loss: {loss:.4f}, Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model name if you want it updated\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to better visualize predictions. Otherwise, things seem blurry.\n",
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X_test, y_test, num_samples=3, upscale_factor=1):\n",
    "    predictions = model.predict(X_test[:num_samples])\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_resized = cv2.resize(gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_resized = gt\n",
    "\n",
    "        gt_mask = gt_resized > 0\n",
    "        gt_display = np.zeros_like(gt_resized)\n",
    "        gt_display[gt_mask] = normalize_for_display(gt_resized[gt_mask])\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('Predicted Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, X_test_2, y_test_2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta error calculation (non-quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original Keras model - uncomment this line if you want to test your saved model against the test set.\n",
    "# model = tf.keras.models.load_model(\"model.h5\", custom_objects={'berHu_loss':berHu_loss})\n",
    "\n",
    "predictions = model.predict(X_test_2, batch_size=16, verbose=1)\n",
    "\n",
    "# Constants (adjust to match your dataset / camera model if not using KITTI)\n",
    "focal_length = 721.5377\n",
    "baseline = 0.5327\n",
    "\n",
    "def disparity_to_depth(disp):\n",
    "    return focal_length * baseline / np.clip(disp, 1e-6, None)\n",
    "\n",
    "def crop_for_eigen(gt):\n",
    "    h, w = gt.shape\n",
    "    crop = np.zeros_like(gt, dtype=bool)\n",
    "    crop[int(0.4081 * h):int(0.9919 * h), int(0.0359 * w):int(0.9640 * w)] = True\n",
    "    return crop\n",
    "\n",
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "for i in range(len(y_test_2)):\n",
    "    gt_disp = y_test_2[i].squeeze()\n",
    "    pred_disp = predictions[i].squeeze()\n",
    "\n",
    "    gt_depth = disparity_to_depth(gt_disp)\n",
    "    pred_depth = disparity_to_depth(pred_disp)\n",
    "\n",
    "    valid_mask = (gt_depth > 0) & crop_for_eigen(gt_depth)\n",
    "\n",
    "    gt = gt_depth[valid_mask]\n",
    "    pred = pred_depth[valid_mask]\n",
    "\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line if you want to use this logic to fully quantize your own model.\n",
    "# model = tf.keras.models.load_model(\"model.h5\", custom_objects={'berHu_loss':berHu_loss})\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "def representative_dataset():\n",
    "    # Take 100 random samples from X_train for calibration - otherwise, TFLite will refuse the quantization to int8\n",
    "    num_samples = min(100, len(X_train_2))\n",
    "    indices = np.random.choice(len(X_train_2), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = X_train_2[idx:idx+1]\n",
    "        yield [sample]\n",
    "\n",
    "converter.representative_dataset = representative_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the file for more clarity.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with .tflite model on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"\") # Put the path to your TFLite model here\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "input_dtype = input_details[0]['dtype']\n",
    "output_dtype = output_details[0]['dtype']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, img in enumerate(X_test_2):\n",
    "    # Quantize input if needed - I used uint8 for input and output, change this if you used int8.\n",
    "    if input_dtype == np.uint8:\n",
    "        input_data = (img / input_scale + input_zero_point).astype(np.uint8)\n",
    "    else:\n",
    "        input_data = img.astype(input_dtype)\n",
    "\n",
    "    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n",
    "\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "\n",
    "    # Dequantize if needed - I used uint8 for input and output, change this if you used int8.\n",
    "    if output_dtype == np.uint8:\n",
    "        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "    predictions.append(output_data[0])\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)\n",
    "\n",
    "def visualize_tflite_predictions(X_test, y_test, predictions, num_samples=3, upscale_factor=1):\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "        gt_display = normalize_for_display(gt)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_display = cv2.resize(gt_display, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('TFLite Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tflite_predictions(X_test_2, y_test_2, predictions, num_samples=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Error Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust to match your dataset or camera model if not using KITTI)\n",
    "focal_length = 721.5377\n",
    "baseline = 0.5327\n",
    "\n",
    "def disparity_to_depth(disp):\n",
    "    return focal_length * baseline / np.clip(disp, 1e-6, None)\n",
    "\n",
    "def crop_for_eigen(gt):\n",
    "    h, w = gt.shape\n",
    "    crop = np.zeros_like(gt, dtype=bool)\n",
    "    crop[int(0.4081 * h):int(0.9919 * h), int(0.0359 * w):int(0.9640 * w)] = True\n",
    "    return crop\n",
    "\n",
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "delta_low_all = []\n",
    "delta_mid_all = []\n",
    "delta_high_all = []\n",
    "\n",
    "for i in range(len(y_test_2)):\n",
    "    gt_disp = y_test_2[i].squeeze()\n",
    "    pred_disp = predictions[i].squeeze()\n",
    "\n",
    "    gt_depth = disparity_to_depth(gt_disp)\n",
    "    pred_depth = disparity_to_depth(pred_disp)\n",
    "\n",
    "    valid_mask = (gt_depth > 0) & crop_for_eigen(gt_depth)\n",
    "\n",
    "    gt = gt_depth[valid_mask]\n",
    "    pred = pred_depth[valid_mask]\n",
    "    disp = gt_disp[valid_mask]\n",
    "\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "    sorted_indices = np.argsort(disp)\n",
    "    n = len(disp)\n",
    "    thirds = n // 3\n",
    "\n",
    "    low_idx = sorted_indices[:thirds]\n",
    "    mid_idx = sorted_indices[thirds:2*thirds]\n",
    "    high_idx = sorted_indices[2*thirds:]\n",
    "\n",
    "    delta_low_all.append(np.mean(delta[low_idx] < 1.25))\n",
    "    delta_mid_all.append(np.mean(delta[mid_idx] < 1.25))\n",
    "    delta_high_all.append(np.mean(delta[high_idx] < 1.25))\n",
    "\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "delta_low = np.mean(delta_low_all) * 100\n",
    "delta_mid = np.mean(delta_mid_all) * 100\n",
    "delta_high = np.mean(delta_high_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\\n\")\n",
    "\n",
    "print(\"Delta < 1.25 by Disparity Region:\")\n",
    "print(f\"  Low disparity (far):    {delta_low:.2f}%\")\n",
    "print(f\"  Mid disparity:          {delta_mid:.2f}%\")\n",
    "print(f\"  High disparity (close): {delta_high:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal μPyD‑Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temporal_frames_from_split(split_file_path, data_root, target_size=(64, 64), original_width=1242, time_steps=3):\n",
    "    \"\"\"\n",
    "    For each frame in the split file, finds the previous (time_steps - 1) adjacent frames (by filename).\n",
    "    Builds input/output pairs suitable for LSTM models.\n",
    "    Returns:\n",
    "        - image_seqs: (N, T, H, W, 1)\n",
    "        - disp_seqs:  (N, T, H, W, 1)\n",
    "    \"\"\"\n",
    "    assert time_steps >= 1, \"time_steps must be >= 1\"\n",
    "    with open(split_file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    image_seqs = []\n",
    "    disp_seqs = []\n",
    "    scale_factor = target_size[1] / original_width\n",
    "\n",
    "    for line in lines:\n",
    "        left_path = line.strip().split()[0]\n",
    "        parts = Path(left_path).parts[1:]\n",
    "        rel_path = Path(*parts)\n",
    "\n",
    "        seq = rel_path.parts[0]\n",
    "        frame_id_str = rel_path.stem\n",
    "        frame_id = int(frame_id_str)\n",
    "\n",
    "        frames = []\n",
    "        for offset in reversed(range(time_steps)):\n",
    "            frame_index = frame_id - offset\n",
    "            if frame_index < 0:\n",
    "                break  # Have not found 2 frames before this\n",
    "            frame_name = f\"{frame_index:010d}\"\n",
    "\n",
    "            image_path = Path(data_root) / seq / \"images\" / f\"{frame_name}.png\"\n",
    "            disp_path = Path(data_root) / seq / \"depths\" / f\"{frame_name}.npy\"\n",
    "\n",
    "            if not image_path.exists() or not disp_path.exists():\n",
    "                break\n",
    "\n",
    "            img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                break\n",
    "\n",
    "            img = cv2.resize(img, target_size).astype(np.float32) / 255.0\n",
    "            img = img[..., np.newaxis]\n",
    "\n",
    "            disp = np.load(disp_path)\n",
    "            disp = cv2.resize(disp, target_size, interpolation=cv2.INTER_NEAREST).astype(np.float32)\n",
    "            disp *= scale_factor\n",
    "            disp = disp[..., np.newaxis]\n",
    "\n",
    "            frames.append((img, disp))\n",
    "\n",
    "        if len(frames) == time_steps:\n",
    "            imgs, disps = zip(*frames)\n",
    "            image_seqs.append(np.stack(imgs, axis=0))\n",
    "            disp_seqs.append(np.stack(disps, axis=0))\n",
    "\n",
    "    return np.array(image_seqs), np.array(disp_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r\"\" # This should be the path to your preprocessed dataset (KITTI or anything else, as long as it maintains folder structure)\n",
    "\n",
    "X_train, y_train_seq = load_temporal_frames_from_split(\"eigen_train_files.txt\", data_root, target_size=(32, 32))\n",
    "y_train = y_train_seq[:, -1]\n",
    "\n",
    "X_val, y_val_seq = load_temporal_frames_from_split(\"eigen_val_files.txt\", data_root, target_size=(32, 32))\n",
    "y_val = y_val_seq[:, -1]\n",
    "\n",
    "X_test, y_test_seq = load_temporal_frames_from_split(\"eigen_test_files.txt\", data_root, target_size=(32, 32))\n",
    "y_test = y_test_seq[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pico-compatible Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, Conv2DTranspose, TimeDistributed, LeakyReLU, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_uPyDNet_tflm(input_shape=(3, 32, 32, 1)):\n",
    "\n",
    "    def conv_block_td(x, filters, stride=1):\n",
    "        x = TimeDistributed(Conv2D(filters, 3, strides=stride, padding='same'))(x)\n",
    "        x = TimeDistributed(LeakyReLU(alpha=0.125))(x)\n",
    "        return x\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    skips = []\n",
    "\n",
    "    x = conv_block_td(inputs, 8, stride=1)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block_td(x, 8, stride=2)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block_td(x, 16, stride=1)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block_td(x, 16, stride=2)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = conv_block_td(x, 32, stride=1)\n",
    "\n",
    "    x = Lambda(lambda t: K.mean(t, axis=1))(x)\n",
    "\n",
    "    decoder_channels = [32, 16]\n",
    "\n",
    "    for i in reversed(range(2)):\n",
    "        x = Conv2DTranspose(decoder_channels[i], 2, strides=2, padding='same')(x)\n",
    "\n",
    "        skip = skips[i * 2]\n",
    "        skip = Lambda(lambda s: s[:, -1, :, :, :])(skip)\n",
    "\n",
    "        x = Concatenate(axis=-1)([x, skip])\n",
    "        x = Conv2D(decoder_channels[i], 3, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.125)(x)\n",
    "        x = Conv2D(decoder_channels[i], 3, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.125)(x)\n",
    "\n",
    "    output = Conv2D(1, 3, padding='same')(x)\n",
    "    return Model(inputs, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_temporal_uPyDNet_tflm()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=berHu_loss, metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "color_map = defaultdict(dict)\n",
    "color_map[Conv2D]['fill'] = 'skyblue'\n",
    "color_map[Conv2DTranspose]['fill'] = 'lightgreen'\n",
    "color_map[LeakyReLU]['fill'] = 'orange'\n",
    "color_map[Concatenate]['fill'] = 'pink'\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, color_map=color_map, to_file='lstm_upyDNet_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('model_checkpoint.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest loss: {loss:.4f}, Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temporal_predictions(model, X_seq_test, y_seq_test, num_samples=3, upscale_factor=1):\n",
    "    \"\"\"\n",
    "    Visualize predictions from an LSTM-based depth model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained temporal Keras model\n",
    "        X_seq_test: Input sequences, shape (N, T, H, W, 1)\n",
    "        y_seq_test: Ground truth disparity maps, shape (N, H, W, 1)\n",
    "        num_samples: Number of samples to visualize\n",
    "        upscale_factor: Factor to enlarge images for display\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_seq_test[:num_samples])\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_seq_test[i, -1].squeeze()\n",
    "        gt = y_seq_test[i].squeeze()\n",
    "        pred = predictions[i].squeeze()\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_resized = cv2.resize(gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_resized = gt\n",
    "\n",
    "        gt_mask = gt_resized > 0\n",
    "        gt_valid = np.where(gt_mask, gt_resized, np.nan)\n",
    "        gt_display = normalize_for_display(np.nan_to_num(gt_valid, nan=0.0))\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image (last frame)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('Predicted Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_temporal_predictions(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size=16, verbose=1)\n",
    "\n",
    "# Constants (adjust to match your dataset or camera model if not using KITTI)\n",
    "focal_length = 721.5377\n",
    "baseline = 0.5327\n",
    "\n",
    "def disparity_to_depth(disp):\n",
    "    return focal_length * baseline / np.clip(disp, 1e-6, None)\n",
    "\n",
    "def crop_for_eigen(gt):\n",
    "    h, w = gt.shape\n",
    "    crop = np.zeros_like(gt, dtype=bool)\n",
    "    crop[int(0.4081 * h):int(0.9919 * h), int(0.0359 * w):int(0.9640 * w)] = True\n",
    "    return crop\n",
    "\n",
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    gt_disp = y_test[i].squeeze()\n",
    "    pred_disp = predictions[i].squeeze()\n",
    "\n",
    "    gt_depth = disparity_to_depth(gt_disp)\n",
    "    pred_depth = disparity_to_depth(pred_disp)\n",
    "\n",
    "    valid_mask = (gt_depth > 0) & crop_for_eigen(gt_depth)\n",
    "\n",
    "    gt = gt_depth[valid_mask]\n",
    "    pred = pred_depth[valid_mask]\n",
    "\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you want to convert your own model using this code.\n",
    "# model = tf.keras.models.load_model(\"model.h5\", custom_objects={'berHu_loss':berHu_loss})\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "def representative_dataset():\n",
    "    num_samples = min(100, len(X_train))\n",
    "    indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = X_train[idx:idx+1]\n",
    "        yield [sample]\n",
    "\n",
    "converter.representative_dataset = representative_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"\") # Replace this with the path to you TFLite model\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "input_dtype = input_details[0]['dtype']\n",
    "output_dtype = output_details[0]['dtype']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, img in enumerate(X_test):\n",
    "    # Quantize input if needed - I used uint8 for my input and output layers, you need to change this to int8 if you used that instead.\n",
    "    if input_dtype == np.uint8:\n",
    "        input_data = (img / input_scale + input_zero_point).astype(np.uint8)\n",
    "    else:\n",
    "        input_data = img.astype(input_dtype)\n",
    "\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "\n",
    "    # Dequantize if needed - I used uint8 for my input and output layers, you need to change this to int8 if you used that instead.\n",
    "    if output_dtype == np.uint8:\n",
    "        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "    predictions.append(output_data[0])\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temporal_predictions(predictions, y_seq_test, num_samples=3, upscale_factor=1):\n",
    "    \"\"\"\n",
    "    Visualize predictions from an LSTM-based depth model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained temporal Keras model\n",
    "        X_seq_test: Input sequences, shape (N, T, H, W, 1)\n",
    "        y_seq_test: Ground truth disparity maps, shape (N, H, W, 1)\n",
    "        num_samples: Number of samples to visualize\n",
    "        upscale_factor: Factor to enlarge images for display\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, -1].squeeze()\n",
    "        gt = y_seq_test[i].squeeze()\n",
    "        pred = predictions[i].squeeze()\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_resized = cv2.resize(gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_resized = gt\n",
    "\n",
    "        gt_mask = gt_resized > 0\n",
    "        gt_valid = np.where(gt_mask, gt_resized, np.nan)\n",
    "        gt_display = normalize_for_display(np.nan_to_num(gt_valid, nan=0.0))\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image (last frame)')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('Predicted Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_temporal_predictions(predictions, y_test, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (adjust to match your dataset or camera model if not using KITTI)\n",
    "focal_length = 721.5377\n",
    "baseline = 0.5327\n",
    "\n",
    "def disparity_to_depth(disp):\n",
    "    return focal_length * baseline / np.clip(disp, 1e-6, None)\n",
    "\n",
    "def crop_for_eigen(gt):\n",
    "    h, w = gt.shape\n",
    "    crop = np.zeros_like(gt, dtype=bool)\n",
    "    crop[int(0.4081 * h):int(0.9919 * h), int(0.0359 * w):int(0.9640 * w)] = True\n",
    "    return crop\n",
    "\n",
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "delta_low_all = []\n",
    "delta_mid_all = []\n",
    "delta_high_all = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    gt_disp = y_test[i].squeeze()\n",
    "    pred_disp = predictions[i].squeeze()\n",
    "\n",
    "    gt_depth = disparity_to_depth(gt_disp)\n",
    "    pred_depth = disparity_to_depth(pred_disp)\n",
    "\n",
    "    valid_mask = (gt_depth > 0) & crop_for_eigen(gt_depth)\n",
    "\n",
    "    gt = gt_depth[valid_mask]\n",
    "    pred = pred_depth[valid_mask]\n",
    "    disp = gt_disp[valid_mask]\n",
    "\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "    sorted_indices = np.argsort(disp)\n",
    "    n = len(disp)\n",
    "    thirds = n // 3\n",
    "\n",
    "    low_idx = sorted_indices[:thirds]\n",
    "    mid_idx = sorted_indices[thirds:2*thirds]\n",
    "    high_idx = sorted_indices[2*thirds:]\n",
    "\n",
    "    delta_low_all.append(np.mean(delta[low_idx] < 1.25))\n",
    "    delta_mid_all.append(np.mean(delta[mid_idx] < 1.25))\n",
    "    delta_high_all.append(np.mean(delta[high_idx] < 1.25))\n",
    "\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "delta_low = np.mean(delta_low_all) * 100\n",
    "delta_mid = np.mean(delta_mid_all) * 100\n",
    "delta_high = np.mean(delta_high_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\\n\")\n",
    "\n",
    "print(\"Delta < 1.25 by Disparity Region:\")\n",
    "print(f\"  Low disparity (far):    {delta_low:.2f}%\")\n",
    "print(f\"  Mid disparity:          {delta_mid:.2f}%\")\n",
    "print(f\"  High disparity (close): {delta_high:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-EfficientUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import DepthwiseConv2D, BatchNormalization, ReLU, MaxPooling2D, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_separable_conv(x, filters, kernel_size=3, strides=1):\n",
    "    x = DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(x, filters):\n",
    "    x = depthwise_separable_conv(x, filters)\n",
    "    skip = x\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x, skip\n",
    "\n",
    "def decoder_block(x, skip, filters):\n",
    "    x = UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = depthwise_separable_conv(x, filters)\n",
    "    return x\n",
    "\n",
    "def build_l_efficientunet(input_shape=(64, 64, 3), num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x, skip1 = encoder_block(inputs, 16)\n",
    "    x, skip2 = encoder_block(x, 32)\n",
    "    x, skip3 = encoder_block(x, 64)\n",
    "    x, skip4 = encoder_block(x, 128)\n",
    "\n",
    "    x = depthwise_separable_conv(x, 256)\n",
    "\n",
    "    x = decoder_block(x, skip4, 128)\n",
    "    x = decoder_block(x, skip3, 64)\n",
    "    x = decoder_block(x, skip2, 32)\n",
    "    x = decoder_block(x, skip1, 16)\n",
    "\n",
    "    outputs = Conv2D(num_classes, kernel_size=1)(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name='L_EfficientUNet')\n",
    "    print(f\"\\nTotal params: {model.count_params():,}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_l_efficientunet(input_shape=(32, 32, 1), num_classes=1)\n",
    "model.compile(optimizer=Adam(1e-4), loss=berHu_loss, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "color_map = defaultdict(dict)\n",
    "color_map[Conv2D]['fill'] = 'skyblue'\n",
    "color_map[Conv2DTranspose]['fill'] = 'lightgreen'\n",
    "color_map[LeakyReLU]['fill'] = 'orange'\n",
    "color_map[Concatenate]['fill'] = 'pink'\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, color_map=color_map, to_file='l-efficientUNet_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_2, y_train_2,\n",
    "    validation_data=(X_val_2, y_val_2),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae = model.evaluate(X_test_2, y_test_2)\n",
    "print(f\"\\nTest loss: {loss:.4f}, Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X_test, y_test, num_samples=3, upscale_factor=1):\n",
    "    predictions = model.predict(X_test[:num_samples])\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_resized = cv2.resize(gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_resized = gt\n",
    "\n",
    "        gt_mask = gt_resized > 0\n",
    "        gt_display = np.zeros_like(gt_resized)\n",
    "        gt_display[gt_mask] = normalize_for_display(gt_resized[gt_mask])\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('Predicted Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, X_test_2, y_test_2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line if you want to use this code to quantize your own model.\n",
    "# model = tf.keras.models.load_model(\"model.h5\", custom_objects={'berHu_loss':berHu_loss})\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "def representative_dataset():\n",
    "    num_samples = min(100, len(X_train_2))\n",
    "    indices = np.random.choice(len(X_train_2), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = X_train_2[idx:idx+1]\n",
    "        yield [sample]\n",
    "\n",
    "converter.representative_dataset = representative_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"\") # Replace this with the path to your TFLite model\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "input_dtype = input_details[0]['dtype']\n",
    "output_dtype = output_details[0]['dtype']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, img in enumerate(X_test_2):\n",
    "    # Quantize input if needed - I used uint8 for my input and output, if you used int8, change the below code accordingly.\n",
    "    if input_dtype == np.uint8:\n",
    "        input_data = (img / input_scale + input_zero_point).astype(np.uint8)\n",
    "    else:\n",
    "        input_data = img.astype(input_dtype)\n",
    "\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "\n",
    "    # Dequantize if needed - I used uint8 for my input and output, if you used int8, change the below code accordingly.\n",
    "    if output_dtype == np.uint8:\n",
    "        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "    predictions.append(output_data[0])\n",
    "\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)\n",
    "\n",
    "def visualize_tflite_predictions(X_test, y_test, predictions, num_samples=3, upscale_factor=1):\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "        gt_display = normalize_for_display(gt)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_display = cv2.resize(gt_display, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('TFLite Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tflite_predictions(X_test_2, y_test_2, predictions, num_samples=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    # Extract predicted and ground truth disparities\n",
    "    gt = y_test_2[i].squeeze()\n",
    "    pred = predictions[i].squeeze()\n",
    "\n",
    "    # Mask out invalid ground truth pixels\n",
    "    valid_mask = gt > 0\n",
    "\n",
    "    gt = gt[valid_mask]\n",
    "    pred = pred[valid_mask]\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "# Convert to percentages\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pico friendly L-ENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_separable_conv(x, out_channels, stride=1):\n",
    "    x = DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(max_value=6)(x)\n",
    "    x = Conv2D(out_channels, 1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(max_value=6)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def initial_block(x, out_channels):\n",
    "    conv = Conv2D(out_channels - 3, 3, strides=2, padding='same')(x)\n",
    "    pool = MaxPooling2D(pool_size=2, strides=2, padding='same')(x)\n",
    "    return Concatenate()([conv, pool])\n",
    "\n",
    "\n",
    "def bottleneck(x, out_channels, downsample=False):\n",
    "    residual = x\n",
    "    stride = 2 if downsample else 1\n",
    "\n",
    "    x = depthwise_separable_conv(x, out_channels, stride=stride)\n",
    "\n",
    "    if downsample:\n",
    "        residual = MaxPooling2D(pool_size=2, strides=2, padding='same')(residual)\n",
    "\n",
    "    if residual.shape[-1] != out_channels:\n",
    "        residual = Conv2D(out_channels, 1, padding='same')(residual)\n",
    "\n",
    "    x = Add()([x, residual])\n",
    "    x = ReLU(max_value=6)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample_block(x, skip, out_channels):\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(out_channels, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    if x.shape[1:3] == skip.shape[1:3]:\n",
    "        x = Concatenate()([x, skip])\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_lenet_fixed(input_shape=(64, 64, 3), num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = initial_block(inputs, out_channels=12)\n",
    "    skip1 = x\n",
    "\n",
    "    x = bottleneck(x, 16, downsample=True)\n",
    "    skip2 = x\n",
    "\n",
    "    x = bottleneck(x, 24, downsample=True)\n",
    "    skip3 = x\n",
    "\n",
    "    x = bottleneck(x, 32, downsample=True)\n",
    "\n",
    "    x = bottleneck(x, 32)\n",
    "\n",
    "    x = upsample_block(x, skip3, 24)\n",
    "    x = upsample_block(x, skip2, 16)\n",
    "    x = upsample_block(x, skip1, 12)\n",
    "\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(12, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    outputs = Conv2D(num_classes, 1, padding='same')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lenet_fixed(input_shape=(64, 64, 1), num_classes=1)\n",
    "\n",
    "model.compile(optimizer='adam', loss=berHu_loss, metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "color_map = defaultdict(dict)\n",
    "color_map[Conv2D]['fill'] = 'skyblue'\n",
    "color_map[Conv2DTranspose]['fill'] = 'lightgreen'\n",
    "color_map[LeakyReLU]['fill'] = 'orange'\n",
    "color_map[Concatenate]['fill'] = 'pink'\n",
    "\n",
    "# Create the visual representation\n",
    "visualkeras.layered_view(model, legend=True, color_map=color_map, to_file='l-ENet_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=80, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_2, y_train_2,\n",
    "    validation_data=(X_val_2, y_val_2),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae = model.evaluate(X_test_2, y_test_2)\n",
    "print(f\"\\nTest loss: {loss:.4f}, Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X_test, y_test, num_samples=3, upscale_factor=1):\n",
    "    predictions = model.predict(X_test[:num_samples])\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_resized = cv2.resize(gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            gt_resized = gt\n",
    "\n",
    "        gt_mask = gt_resized > 0\n",
    "        gt_display = np.zeros_like(gt_resized)\n",
    "        gt_display[gt_mask] = normalize_for_display(gt_resized[gt_mask])\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('Predicted Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, X_test_2, y_test_2, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you want to use this code to quantize your own model\n",
    "# model = tf.keras.models.load_model(\"model.h5\", custom_objects={'berHu_loss':berHu_loss})\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "def representative_dataset():\n",
    "    num_samples = min(100, len(X_train_2))\n",
    "    indices = np.random.choice(len(X_train_2), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = X_train_2[idx:idx+1]\n",
    "        yield [sample]\n",
    "\n",
    "converter.representative_dataset = representative_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"\") # Replace this with a path to your TFLite model\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[0]['quantization']\n",
    "\n",
    "input_dtype = input_details[0]['dtype']\n",
    "output_dtype = output_details[0]['dtype']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, img in enumerate(X_test_2):\n",
    "    # Quantize input if needed - I used uint8 for my input and output layers, if you used int8, modify the code below accordingly.\n",
    "    if input_dtype == np.uint8:\n",
    "        input_data = (img / input_scale + input_zero_point).astype(np.uint8)\n",
    "    else:\n",
    "        input_data = img.astype(input_dtype)\n",
    "\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "\n",
    "    # Dequantize if needed - I used uint8 for my input and output layers, if you used int8, modify the code below accordingly.\n",
    "    if output_dtype == np.uint8:\n",
    "        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "    predictions.append(output_data[0])\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_for_display(arr, clip_percentile=2):\n",
    "    vmin = np.percentile(arr, clip_percentile)\n",
    "    vmax = np.percentile(arr, 100 - clip_percentile)\n",
    "    arr_clipped = np.clip(arr, vmin, vmax)\n",
    "    normed = (arr_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "    return (normed * 255).astype(np.uint8)\n",
    "\n",
    "def visualize_tflite_predictions(X_test, y_test, predictions, num_samples=3, upscale_factor=1):\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_img = X_test[i, ..., 0]\n",
    "        gt = y_test[i, ..., 0]\n",
    "        pred = predictions[i, ..., 0]\n",
    "\n",
    "        pred_display = normalize_for_display(pred)\n",
    "        gt_display = normalize_for_display(gt)\n",
    "\n",
    "        if upscale_factor > 1:\n",
    "            h, w = input_img.shape\n",
    "            new_size = (w * upscale_factor, h * upscale_factor)\n",
    "            input_img = cv2.resize(input_img, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            pred_display = cv2.resize(pred_display, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "            gt_display = cv2.resize(gt_display, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Input image\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(input_img, cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Ground truth\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(gt_display, cmap='magma')\n",
    "        plt.title('Ground Truth Depth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Prediction\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_display, cmap='magma')\n",
    "        plt.title('TFLite Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tflite_predictions(X_test_2, y_test_2, predictions, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta error evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1_all = []\n",
    "delta_2_all = []\n",
    "delta_3_all = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    gt = y_test_2[i].squeeze()\n",
    "    pred = predictions[i].squeeze()\n",
    "\n",
    "    valid_mask = gt > 0\n",
    "\n",
    "    gt = gt[valid_mask]\n",
    "    pred = pred[valid_mask]\n",
    "\n",
    "    pred = np.clip(pred, 1e-6, None)\n",
    "\n",
    "    delta = np.maximum(pred / gt, gt / pred)\n",
    "\n",
    "    delta_1_all.append(np.mean(delta < 1.25))\n",
    "    delta_2_all.append(np.mean(delta < 1.25 ** 2))\n",
    "    delta_3_all.append(np.mean(delta < 1.25 ** 3))\n",
    "\n",
    "delta_1 = np.mean(delta_1_all) * 100\n",
    "delta_2 = np.mean(delta_2_all) * 100\n",
    "delta_3 = np.mean(delta_3_all) * 100\n",
    "\n",
    "print(f\"Delta < 1.25: {delta_1:.2f}%\")\n",
    "print(f\"Delta < 1.25²: {delta_2:.2f}%\")\n",
    "print(f\"Delta < 1.25³: {delta_3:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion logic for model to .cpp array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tflite_to_c_array(tflite_path, output_cpp_path, array_name=\"model\"):\n",
    "    with open(tflite_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    with open(output_cpp_path, \"w\") as f:\n",
    "        f.write(f\"const unsigned char {array_name}[] = {{\\n\")\n",
    "        for i, byte in enumerate(data):\n",
    "            if i % 12 == 0:\n",
    "                f.write(\"  \")\n",
    "            f.write(f\"0x{byte:02x}, \")\n",
    "            if (i + 1) % 12 == 0:\n",
    "                f.write(\"\\n\")\n",
    "        f.write(f\"\\n}};\\n\")\n",
    "        f.write(f\"const unsigned int {array_name}_len = {len(data)};\\n\")\n",
    "\n",
    "    print(f\"Saved C array to {output_cpp_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_path = r\"\" # Change this to your TFLite model path\n",
    "\n",
    "convert_tflite_to_c_array(tflite_path, \"model.cpp\", \"tflite_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to array conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(r\"\") # This should be a path to your preprocessed data folder.\n",
    "split_file = Path(r\"\") # This should be a path to your test set paths.\n",
    "OUTPUT_CPP = \"test_image_data.cpp\"\n",
    "ARRAY_NAME = \"g_test_image_data\"\n",
    "WIDTH, HEIGHT = 32, 32\n",
    "INDEX = 0 # Change this index to reflect the image you want to convert.\n",
    "\n",
    "with open(split_file, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "line = lines[INDEX]\n",
    "left_path = line.strip().split()[0]\n",
    "parts = Path(left_path).parts[1:]\n",
    "rel_path = Path(*parts)\n",
    "\n",
    "sequence = rel_path.parts[0]\n",
    "filename = rel_path.stem\n",
    "\n",
    "image_path = data_root / sequence / \"images\" / f\"{filename}.png\"\n",
    "\n",
    "if not image_path.exists():\n",
    "    raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    raise RuntimeError(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "img_resized = cv2.resize(img, (WIDTH, HEIGHT)).astype(np.float32) / 255.0\n",
    "\n",
    "img_quantized = np.clip(np.round(img_resized * 255.0), 0, 255).astype(np.uint8).flatten()\n",
    "\n",
    "with open(OUTPUT_CPP, \"w\") as f:\n",
    "    f.write('#include <cstdint>\\n\\n')\n",
    "    f.write(f'const unsigned int {ARRAY_NAME}_size = {len(img_quantized)};\\n')\n",
    "    f.write(f'alignas(16) const unsigned char {ARRAY_NAME}[] = {{\\n')\n",
    "\n",
    "    for i, val in enumerate(img_quantized):\n",
    "        f.write(f'0x{val:02x}, ')\n",
    "        if (i + 1) % 12 == 0:\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.write('\\n};\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image sequence to array conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(r\"\") # This should be a path to your preprocessed data folder.\n",
    "split_file = Path(r\"\") # This should be a path to your test set paths.\n",
    "OUTPUT_CPP = \"test_sequence_data.cpp\"\n",
    "ARRAY_NAME = \"g_test_sequence_data\"\n",
    "WIDTH, HEIGHT = 32, 32\n",
    "TIME_STEPS = 3 # This should be the length of the sequence used in your model\n",
    "TARGET_INDEX = -1 # Change this to an available index, >= TIME_STEPS - 1. Otherwise, the code will fail.\n",
    "\n",
    "with open(split_file, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "line = lines[TARGET_INDEX]\n",
    "left_path = line.strip().split()[0]\n",
    "parts = Path(left_path).parts[1:]\n",
    "rel_path = Path(*parts)\n",
    "\n",
    "seq = rel_path.parts[0]\n",
    "frame_id = int(rel_path.stem)\n",
    "\n",
    "frames = []\n",
    "for offset in reversed(range(TIME_STEPS)):\n",
    "    frame_index = frame_id - offset\n",
    "    if frame_index < 0:\n",
    "        raise ValueError(\"Not enough previous frames for temporal sequence.\")\n",
    "\n",
    "    frame_name = f\"{frame_index:010d}\"\n",
    "    image_path = data_root / seq / \"images\" / f\"{frame_name}.png\"\n",
    "\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Failed to load: {image_path}\")\n",
    "\n",
    "    img_resized = cv2.resize(img, (WIDTH, HEIGHT)).astype(np.float32) / 255.0\n",
    "    img_quantized = np.clip(np.round(img_resized * 255.0), 0, 255).astype(np.uint8)\n",
    "    frames.append(img_quantized)\n",
    "\n",
    "sequence_array = np.stack(frames, axis=0).flatten()\n",
    "\n",
    "with open(OUTPUT_CPP, \"w\") as f:\n",
    "    f.write('#include <cstdint>\\n\\n')\n",
    "    f.write(f'const unsigned int {ARRAY_NAME}_size = {len(sequence_array)};\\n')\n",
    "    f.write(f'alignas(16) const unsigned char {ARRAY_NAME}[] = {{\\n')\n",
    "\n",
    "    for i, val in enumerate(sequence_array):\n",
    "        f.write(f'0x{val:02x}, ')\n",
    "        if (i + 1) % 12 == 0:\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.write('\\n};\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
